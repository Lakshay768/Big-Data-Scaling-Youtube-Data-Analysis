{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "View_count_modelling.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0-YhEpP_Ds-"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zsj5WYpR9QId"
      },
      "source": [
        "Let's setup Spark on  Colab environment.  Run the cell below!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-qHai2252mI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d12a2a0-796f-4aa7-cb0b-00c81716bd1e"
      },
      "source": [
        "!pip install pyspark\n",
        "!apt update\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.1.1)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9)\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:5 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Ign:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:14 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,759 kB]\n",
            "Fetched 1,775 kB in 3s (675 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "52 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 2 newly installed, 0 to remove and 52 not upgraded.\n",
            "Need to get 36.5 MB of archives.\n",
            "After this operation, 143 MB of additional disk space will be used.\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "(Reading database ... 160690 files and directories currently installed.)\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u292-b10-0ubuntu1~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u292-b10-0ubuntu1~18.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u292-b10-0ubuntu1~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u292-b10-0ubuntu1~18.04) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u292-b10-0ubuntu1~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u292-b10-0ubuntu1~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwtlO4_m_LbQ"
      },
      "source": [
        "Now we import some of the libraries usually needed by our workload.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twk-K-jilWK7"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q87BMBrNUIN2"
      },
      "source": [
        "Mounting Google drive to access data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkPmdPtiRqBa",
        "outputId": "f4e4af7e-c078-4428-9147-6edbcd52068e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gkclHnlR-xc",
        "outputId": "497c9b59-8f47-489c-81cd-1c29250dbc88"
      },
      "source": [
        "import os\n",
        "cur_path = \"/content/drive/My Drive/Colab Notebooks/big data project/\"\n",
        "os.chdir(cur_path)\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BR_videos_data.csv  FR_videos_data.csv\tJP_videos_data.csv  RU_videos_data.csv\n",
            "CA_videos_data.csv  IN_videos_data.csv\tMX_videos_data.csv  US_videos_data.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtrJlMBt1Ela"
      },
      "source": [
        "Let's initialize the Spark context."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vm3sAVeK1EDZ"
      },
      "source": [
        "# create the session\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
        "\n",
        "# create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqovskkH1DmC"
      },
      "source": [
        "You can easily check the current version and get the link of the web interface. In the Spark UI, you can monitor the progress of your job and debug the performance bottlenecks (if your Colab is running with a **local runtime**)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DueQggJc1DDk"
      },
      "source": [
        "spark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAYRX2PMm0L6"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpsaYOqRxar2"
      },
      "source": [
        "For convenience, given that the dataset is small, we load the spark dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oitav_xhQD9w"
      },
      "source": [
        "#video_data=pd.read_csv('US_videos_data.csv/US_videos_data.csv')\n",
        "df = spark.read.csv('US_videos_data.csv/US_videos_data.csv',header=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvzK_qH6UgnB"
      },
      "source": [
        "Fixing Column types"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ty0JEtM5VNpf"
      },
      "source": [
        "df=df.withColumn(\"view_count\", df[\"view_count\"].cast(\"int\"))\\\n",
        ".withColumn(\"likes\", df[\"likes\"].cast(\"int\"))\\\n",
        ".withColumn(\"dislikes\", df[\"dislikes\"].cast(\"int\"))\\\n",
        ".withColumn(\"comment_count\", df[\"comment_count\"].cast(\"int\"))\\\n",
        ".withColumn(\"trending_date\", to_timestamp(df[\"trending_date\"], \"yy.dd.MM\"))\\\n",
        ".withColumn(\"time_published\", to_timestamp(df[\"publishedAt\"], \"yyyy-MM-dd\"))\\\n",
        ".withColumn(\"categoryId\", df[\"categoryId\"].cast('int'))\\\n",
        ".select('video_id','view_count','title','tags','channelTitle','categoryId','likes','dislikes','comment_count')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmDOHaEbR6dy"
      },
      "source": [
        "## Data for modelling\n",
        "Regression_data=df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYOIVyQqUoMN"
      },
      "source": [
        "Tokenizing the column 'title'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWX-541VYLE0"
      },
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
        "tokenizer = Tokenizer(inputCol=\"title\", outputCol=\"Tokens_title\")\n",
        "df = tokenizer.transform(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCWvQxrZgjhU"
      },
      "source": [
        "df.limit(3).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbinpdUoUwjy"
      },
      "source": [
        "Removing Stop Words from Title"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOo9w2aTYanM"
      },
      "source": [
        "from pyspark.ml.feature import StopWordsRemover\n",
        "remover = StopWordsRemover(inputCol=\"Tokens_title\", outputCol=\"filtered_tokens_title\")\n",
        "df=remover.transform(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1VxaNS2jtvf"
      },
      "source": [
        "df.limit(3).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fylCQOpUh-jd"
      },
      "source": [
        "#### XG boost model to predict view count on a video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmuG_IeEQexu"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.regression import GBTRegressor\n",
        "from pyspark.ml.feature import VectorIndexer\n",
        "from pyspark.ml.evaluation import RegressionEvaluator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEuUSR30SeKg"
      },
      "source": [
        "Regression_data=Regression_data.filter(Regression_data['likes'].isNotNull())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9VGKFdTQ340"
      },
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.clustering import KMeans\n",
        "vec_assembler = VectorAssembler(inputCols = ['categoryId','likes','dislikes','comment_count'], outputCol='features')\n",
        "\n",
        "final_data = vec_assembler.transform(Regression_data)\n",
        "final_data=final_data.select('view_count','features')\n",
        "\n",
        "(trainingData, testData) = final_data.randomSplit([0.8, 0.2])\n",
        "\n",
        "# Train a GBT model.\n",
        "gbt = GBTRegressor(labelCol='view_count', featuresCol=\"features\", maxIter=8)\n",
        "gbt_model= gbt.fit(trainingData)\n",
        "\n",
        "# Make predictions.\n",
        "predictions = gbt_model.transform(testData)\n",
        "\n",
        "# Select (prediction, true label) and compute test error\n",
        "evaluator = RegressionEvaluator(\n",
        "    labelCol=\"view_count\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQPEWzYgUKM8"
      },
      "source": [
        "predictions.select(\"prediction\", \"view_count\", \"features\").show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMYcpy1mWCze"
      },
      "source": [
        "#### Using Generalized Linear method for Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIWTkUZzWBz6"
      },
      "source": [
        "from pyspark.ml.regression import GeneralizedLinearRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucC41rBVWKbe"
      },
      "source": [
        "glr = GeneralizedLinearRegression(family=\"gaussian\", link=\"identity\", maxIter=10, regParam=0.3,labelCol='view_count',featuresCol='features')\n",
        "model= glr.fit(trainingData)\n",
        "predictions = model.transform(testData)\n",
        "\n",
        "# Select (prediction, true label) and compute test error\n",
        "evaluator = RegressionEvaluator(\n",
        "    labelCol=\"view_count\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ku7KT99w54zR"
      },
      "source": [
        "! pip install spark-nlp==3.0.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BErMwhN9_2K"
      },
      "source": [
        "#### Trying out models with Video Titles embedded using Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6edTGyA3kPi"
      },
      "source": [
        "import sparknlp\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.common import *\n",
        "from sparknlp.embeddings import *\n",
        "from pyspark.ml.feature import Word2Vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc4XMt_P634E"
      },
      "source": [
        "Regression_data.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XDhqJ5D7UhO"
      },
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
        "tokenizer = Tokenizer(inputCol=\"title\", outputCol=\"Tokens_title\")\n",
        "Regression_data = tokenizer.transform(Regression_data)\n",
        "\n",
        "\n",
        "from pyspark.ml.feature import StopWordsRemover\n",
        "remover = StopWordsRemover(inputCol=\"Tokens_title\", outputCol=\"filtered_tokens_title\")\n",
        "Regression_data=remover.transform(Regression_data)\n",
        "\n",
        "\n",
        "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"filtered_tokens_title\", outputCol=\"title_embedding\")\n",
        "model = word2Vec.fit(Regression_data)\n",
        "\n",
        "embedded_data = model.transform(Regression_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32dbf2749G5Z"
      },
      "source": [
        "embedded_data.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAYvkPAyW7OD"
      },
      "source": [
        "Creating the new feature vector with title embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMg8hmyu9TDu"
      },
      "source": [
        "vec_assembler = VectorAssembler(inputCols = ['categoryId','likes','dislikes','comment_count','title_embedding'], outputCol='features')\n",
        "\n",
        "final_data = vec_assembler.transform(embedded_data)\n",
        "final_data=final_data.select('view_count','features')\n",
        "\n",
        "(trainingData, testData) = final_data.randomSplit([0.8, 0.2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POkMv5jb91qQ"
      },
      "source": [
        "gbt = GBTRegressor(labelCol='view_count', featuresCol=\"features\", maxIter=8)\n",
        "gbt_model= gbt.fit(trainingData)\n",
        "\n",
        "# Make predictions.\n",
        "predictions = gbt_model.transform(testData)\n",
        "\n",
        "# Select (prediction, true label) and compute test error\n",
        "evaluator = RegressionEvaluator(\n",
        "    labelCol=\"view_count\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hNDRQBK-sWW"
      },
      "source": [
        "gbt_model.featureImportances"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-loEnIhrTI9N"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}